{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF from scratch in Python\n",
    "\n",
    "Concepts explore here are:\n",
    "\n",
    " 1. TFIDF.\n",
    " 2. Similarity search given a **Query** and TFIDF weights.\n",
    " 3. Similarity search with **Cosine similarity** given a Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "\n",
    "import nltk, string, copy, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An apple a day keeps the doctor away.',\n",
       " 'Never compare an apple to an orange.',\n",
       " 'I prefer scikit-learn to orange']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"An apple a day keeps the doctor away.\", \"Never compare an apple to an orange.\",\n",
    " \"I prefer scikit-learn to orange\"]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
    
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = []\n",
    "[processed_docs.append(word_tokenize(str(preprocess(text)))) for text in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = {token:0 for doc in processed_docs for token in doc}\n",
    "vocab = list(DF.keys())\n",
    "for doc in processed_docs:\n",
    "    for token in DF.keys():\n",
    "        if token in doc:\n",
    "            DF[token] +=1\n",
    "            \n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'day'): 0.2772588722239781,\n",
       " (0, 'doctor'): 0.2772588722239781,\n",
       " (0, 'keep'): 0.2772588722239781,\n",
       " (0, 'away'): 0.2772588722239781,\n",
       " (0, 'appl'): 0.18325814637483104,\n",
       " (1, 'never'): 0.34657359027997264,\n",
       " (1, 'appl'): 0.22907268296853878,\n",
       " (1, 'compar'): 0.34657359027997264,\n",
       " (1, 'orang'): 0.22907268296853878,\n",
       " (2, 'orang'): 0.22907268296853878,\n",
       " (2, 'prefer'): 0.34657359027997264,\n",
       " (2, 'scikit'): 0.34657359027997264,\n",
       " (2, 'learn'): 0.34657359027997264}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(processed_docs) # Total number of documents\n",
    "tfidf = {}\n",
    "\n",
    "for i, doc in enumerate(processed_docs):\n",
    "    counter = Counter(doc)\n",
    "    words_count = len(doc)\n",
    "    \n",
    "    for token in set(doc):\n",
    "        tf = counter[token] / words_count\n",
    "        idf = np.log(N / DF[token] + 1)\n",
    "        tfidf[i, token] = tf * idf\n",
    "        \n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking using Matching Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I'd like an apple.\"\n",
    "query_tokens = word_tokenize(str(preprocess(query)))\n",
    "\n",
    "query_weights = {}\n",
    "\n",
    "for k, v in tfidf.items():\n",
    "    if k[1] in query_tokens:\n",
    "        if k[0] in query_weights:\n",
    "            query_weights[k[0]] += v\n",
    "        else: query_weights[k[0]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.18325814637483104, 1: 0.22907268296853878}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Never compare an apple to an orange.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[sorted(query_weights)[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking using Cosine Similarity\n",
    "\n",
    "**Matching score** gives relevant documents but quite fails when we give **long query**.\n",
    "\n",
    "**Cosine similarity** will rank all documents as vectors of **TFIDF** tokens and consider the **angle** between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(a, b):\n",
    "    '''compute consine similarity between two vectors'''\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.zeros((N, vocab_size))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18325815, 0.27725887, 0.27725887, 0.27725887, 0.27725887,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.22907268, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34657359, 0.34657359, 0.22907268, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22907268, 0.34657359, 0.34657359,\n",
       "        0.34657359]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in tfidf.items():\n",
    "    ind = vocab.index(k[1])\n",
    "    D[k[0]][ind] = v\n",
    "    \n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the query to the set of documents and compute the query tfidf score, then generates the query vector for comparison with the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((vocab_size))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46209812, 0.09589402, 0.46209812, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens = word_tokenize(str(preprocess(query)))\n",
    "\n",
    "counter = Counter(query_tokens)\n",
    "words_count = len(query_tokens)\n",
    "\n",
    "for i, token in enumerate(set(query_tokens)):\n",
    "    tf = counter[token] / words_count\n",
    "    df = DF[token] if token in vocab else 0\n",
    "    idf = np.log((N+1) / (df + 1))\n",
    "    Q[i] = tf * idf\n",
    "    \n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = {i:0 for i in range(len(D))}\n",
    "for i, doc_vector in enumerate(D):\n",
    "    cosine_similarity[i] = compute_cosine_similarity(Q, doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6205968869162394, 1: 0.2727800388062073, 2: 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An apple a day keeps the doctor away.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[sorted(cosine_similarity)[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
